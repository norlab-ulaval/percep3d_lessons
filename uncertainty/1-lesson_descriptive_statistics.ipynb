{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;font-size: 40pt\">Descriptive statistics</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "#%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import ipywidgets as widgets\n",
    "plt.style.use('dark_background')\n",
    "\n",
    "%run ./scripts/helper_func.py\n",
    "path = \"{0}/lessons/transformations_2d/scripts/helper_func.py\".format(get_root_path())\n",
    "%run $path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectives of this module:\n",
    "\n",
    "- [re]start easy with the math (or not, but at least start)\n",
    "- introduce part of the notations used in this lecture in a simple context\n",
    "- recall notions of statistic in 1D, which we will build on for 2D and 3D\n",
    "- introduce some basic usage of `numpy` and `matplotlib`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden custom latex commands here $ \\curvearrowright$\n",
    "\n",
    "----\n",
    "[comment]: <> (Useful commands)\n",
    "$\\newcommand{\\textcomma}{\\quad,}$\n",
    "$\\newcommand{\\textdot}{\\quad.}$\n",
    "$\\newcommand{\\vec}[1]{\\mathbf{#1}}$\n",
    "$\\newcommand{\\mat}[1]{\\mathbf{#1}}$\n",
    "$\\newcommand{\\vecsim}[1]{\\boldsymbol{#1}}$\n",
    "$\\newcommand{\\matsim}[1]{\\boldsymbol{#1}}$\n",
    "$\\newcommand{\\real}{\\mathbb{R}}$\n",
    "$\\newcommand{\\E}{\\mathrm{E}}$\n",
    "$\\newcommand{\\L}{\\mathrm{L}}$\n",
    "$\\newcommand{\\Var}{\\mathrm{Var}}$\n",
    "$\\newcommand{\\normal}{\\mathcal{N}}$\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A complex definition of the mean\n",
    "\n",
    "Lets define a set $\\mathcal{X} = \\{x_1, x_2, \\dots, x_i\\}$, with its cardinality being $|\\mathcal{X}| = n $ and $x \\in \\mathbb{R}$.\n",
    "This sentence means that there is $n$ elements in the set $\\mathcal{X}$.\n",
    "\n",
    "We can define the **expectation** of the set $\\mathcal{X}$ using\n",
    "\n",
    "\\begin{equation}\n",
    "\\E[\\mathcal{X}] = \\sum_i^n p_i x_i  \\textcomma\n",
    "\\end{equation}\n",
    "\n",
    "where $p_i$ is the probability associated to a given $x_i$. \n",
    "This can also be understood as a weighted mean. \n",
    "Most of the time, we don't have any prior on the probability of a given $x_i$.\n",
    "By assuming a uniform distribution (i.e., equal chance weight for all $x_i$), we get the well known **arithmetic mean**  defined as\n",
    "\n",
    "\\begin{equation}\n",
    "\\E[\\mathcal{X}] = \\frac{1}{n}\\sum_i^n x_i \\textdot\n",
    "\\end{equation}\n",
    "\n",
    "Note that the expectation is considered as the first **raw** moment.\n",
    "The name \"raw\" comes from the computation being centered to zero (i.e., the origin), which make it describes roughly the location of the set.\n",
    "The general equation of the $k$-th raw moment is\n",
    "\n",
    "\\begin{aligned}\n",
    "\\E[\\mathcal{X}^k] = \\sum_i^n p_i x_i^k  \\textcomma\n",
    "\\end{aligned}\n",
    "\n",
    "but for the context of this lecture, we are only interested in $k=1$.\n",
    "Depending on how authors write their equations, you will find those synonyms for the \n",
    "first raw moment: \n",
    "- mean\n",
    "- expectation\n",
    "- $\\mu$\n",
    "- $\\E[\\mathcal{X}] $\n",
    "- $\\bar{x}$\n",
    "\n",
    "### Example\n",
    "\n",
    "We can do an example where all $x_i$ are draw from a uniform distribution.\n",
    "If all that distribution would be physical weights on a wooden pool, the mean would be the center of mass of those weights.\n",
    "In other words, you could balance the wooden pool on the point representing the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3000\n",
    "uni_start = 1.\n",
    "uni_end = 3.\n",
    "\n",
    "X = np.random.uniform(uni_start, uni_end, n)\n",
    "pretty_print(X)\n",
    "\n",
    "# manual\n",
    "xbar = 0\n",
    "for x in X:\n",
    "    xbar += x # the same as xbar = xbar + x\n",
    "xbar /= n # the same as xbar = xbar/x\n",
    "print(\"mean (manual): \", xbar)\n",
    "\n",
    "# with numpy\n",
    "xbar = np.mean(X)\n",
    "print(\"mean (numpy): \", xbar)\n",
    "\n",
    "#--------------------------------\n",
    "# plotting\n",
    "if 'fig' in globals():\n",
    "    plt.close(fig)\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "n, _, _, = ax.hist(X, bins=50);\n",
    "ax.scatter(xbar, 0, color='tab:red', zorder=3, clip_on=False);\n",
    "ax.legend(['mean'])\n",
    "pretty_ax(ax, \n",
    "          'Histogram of $\\mathcal{X}$', \n",
    "          'Count', \n",
    "          'Values of $x_i$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance as a moment\n",
    "\n",
    "For a set $\\mathcal{X}$, the variance $\\text{Var}(\\mathcal{X})$ corresponds to how much each values vary from the mean $\\E[\\mathcal{X}]$, and is define as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\text{Var}(\\mathcal{X}) = \\E\\left[(\\mathcal{X} -\\E[\\mathcal{X}])^2 \\right] \\textdot\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of a discrete set, the variance can be computed as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\text{Var}(\\mathcal{X}) = \\sum_i^n p_i (x_i - \\bar{x})^2  \\textcomma\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with the same simplification for uniform distribution, similarly as what we did for $\\E[\\mathcal{X}]$, leading to"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\text{Var}(\\mathcal{X}) = \\frac{1}{n}\\sum_i^n (x_i - \\bar{x})^2 \\textdot\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pro trick**: One interesting thing about a summation of squared values is that it can be easily converted to matrix operations, which are usually better optimized in many programation languages.\n",
    "Let convert our set $\\mathcal{X}$ to a vector $\\vec{x} \\in \\real^n$.\n",
    "We can than compute the variance using"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "\\text{Var}(\\mathcal{X}) &= \\frac{1}{n}\\vec{x}^T \\vec{x} - \\bar{x}^2  \\textdot\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some useful properties of the variance:\n",
    "- non negative: $\\text{Var}(\\mathcal{X}) \\geq 0$\n",
    "- translation invariant: $\\text{Var}(\\mathcal{X}) = \\text{Var}(\\mathcal{X}+a)$\n",
    "\n",
    "The variance is also known as the second **central** moment.\n",
    "The name \"central\" comes from that fact that its computation is centered to a raw moment defined in the previous section.\n",
    "Central moments describe more the spread of the set.\n",
    "The general equation of the $k$-th central moment is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    "\\E\\left[ (\\mathcal{X} -\\E[\\mathcal{X}])^k \\right] = \\sum_i^n p_i (x_i - \\bar{x})^k \\textdot\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that these equations represent the **population** covariance equations. We assume that in our case, we can have access to all of the samples of our dataset. In the case of a large dataset where sampling would be necessary, a better alternative is to use **sample** covariance. For this alternative, we know that our sample does not represent the entire population. To compute the sample covariance, we do almost the same as when computing the population covariance except that we divide by $n-1$\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Var}(\\mathcal{X}) = \\frac{1}{n-1}\\sum_i^n (x_i - \\bar{x})^2 \\textdot\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, for this lecture, we will only used the **second central moment**, for which some synonyms can be:\n",
    "- variance\n",
    "- $\\sigma^2$ (not to be confused with $\\sigma$ which is named standard deviation)\n",
    "- $s^2$\n",
    "- $\\text{Var}(\\mathcal{X})$\n",
    "- scale\n",
    "- spread"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "As we did with the mean, we can compute the variance from a uniform distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5000\n",
    "uni_start = 1.\n",
    "uni_end = 3.\n",
    "\n",
    "X = np.random.uniform(uni_start, uni_end, n)\n",
    "pretty_print(X)\n",
    "\n",
    "xbar = np.mean(X)\n",
    "\n",
    "# manual with iterations\n",
    "var = 0\n",
    "for x in X:\n",
    "    var += (x - xbar)**2. # the same as var = var + (x - xbar)^2\n",
    "var /= n # the same as var = var/x\n",
    "print(\"var (loop): \", var)\n",
    "\n",
    "# manual with matrix\n",
    "var = np.dot(X, X)/n - xbar**2.\n",
    "print(\"var (matrix): \", var)\n",
    "\n",
    "# with numpy\n",
    "var = np.var(X)\n",
    "print(\"var (numpy): \", var)\n",
    "\n",
    "sigma = np.sqrt(var)\n",
    "\n",
    "#--------------------------------\n",
    "# plotting\n",
    "if 'fig' in globals():\n",
    "    plt.close(fig)\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "n, _, _, = ax.hist(X, bins=50);\n",
    "ax.plot(xbar, 0, 'o', color='tab:red', clip_on=False, zorder=3);\n",
    "ax.annotate('', xy=(xbar + sigma/2., 0),\n",
    "             xytext=(xbar - sigma/2., 0),\n",
    "             arrowprops=dict(arrowstyle= '|-|', color='tab:red', lw=3)\n",
    "           )\n",
    "ax.text(xbar + sigma/2., np.max(n)/20, '$\\sigma$', color='tab:red', size=20)\n",
    "\n",
    "ax.legend(['mean'])\n",
    "pretty_ax(ax, \n",
    "          'Histogram of $\\mathcal{X}$', \n",
    "          'Count', \n",
    "          'Values of $x_i$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, when plotting a representation of the variance on the original data, we can only show its square root (i.e., the standard deviation or std).\n",
    "Otherwise, the units wouldn't fit.\n",
    "If $x_i$ would be in meters, $\\text{Var}(\\mathcal{X})$ would be in meter squared, but its std would in meter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One more type of moment...\n",
    "\n",
    "The last type of moment is the **standardized moment**.\n",
    "As for the two other types of moments we saw, there is a general definition for the  $k$-th standardized moment, but two most popular are the third standardized moment (i.e., skewness) and the forth standardized moment (i.e.,  kurtosis).\n",
    "Standardized moments are useful as they describe a set in a way that is scale invariant (i.e., invariant to multiply the whole set by a constant (e.g., $a \\mathcal{X} = \\{a x_1, a x_2, \\dots, a x_i\\}$).\n",
    "\n",
    "Skewness gives an idea of how much the set is symmetric, with a skewness of zero meaning perfect symmetry, such as a Gaussian or a uniform distribution.\n",
    "Kurtosis measures the level of outliers in a set.\n",
    "The interpretation of the kurtosis is less intuitive, so people tend to compare with with Normal distribution, which has a kurtosis of 3.\n",
    "So, a kurtosis larger than 3 would have more outliers than a the normal distribution.\n",
    "As an example, the kurtosis of a uniform distribution is 9/5 (i.e., 1.8).\n",
    "\n",
    "We won't use those standardized moments for this lecture, but...\n",
    "<p style=\"text-align:center\">\n",
    "<img src=\"images/themoreyouknow.jpg\" width=30% />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision vs accuracy\n",
    "\n",
    "Being careful between the difference of those terms is useful in engineering, when comparing specification sheets, and in science when comparing your results to other or to ground truth values.\n",
    "Let start with a entertaining example with a robotic dartboard to explain the difference between precision and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('MHTizZ_XcUM', width=720, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will find below a representation of a dartboard with each points being drawn from a normal distribution for the $x$ axis and another one for the $y$ axis.\n",
    "You should play with the sliders to understand:\n",
    "- which of the parameters move the distribution (i.e., location)?\n",
    "- which of the parameters change the size of the distribution (i.e., scale)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 300\n",
    "u_x = 0.1; sigma_x = 0.4\n",
    "u_y = 1.0; sigma_y = 0.2\n",
    "\n",
    "X = np.array([np.random.normal(u_x, sigma_x, n),\n",
    "              np.random.normal(u_y, sigma_y, n)])\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5,5))\n",
    "scat = ax.scatter(X[0,:], X[1,:], zorder=2, alpha = 0.6)\n",
    "\n",
    "draw_dartboard(ax)\n",
    "\n",
    "def update(is_display = True, u_x = 0., sigma_x = 0.1, u_y = 0., sigma_y = 0.1):\n",
    "\n",
    "    if(is_display):\n",
    "        X = np.array([np.random.normal(u_x, sigma_x, n),\n",
    "                      np.random.normal(u_y, sigma_y, n)])\n",
    "        scat.set_offsets(X.T)\n",
    "        scat.set_visible(True)\n",
    "    else:\n",
    "        scat.set_visible(False)\n",
    "        \n",
    "    \n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "widgets.interact(update, u_x = (-2.5, 2.5, 0.1), sigma_x = (0.1, 1.5, 0.1), u_y = (-2.5, 2.5, 0.1), sigma_y = (0.1, 1.5, 0.1));\n",
    "pretty_ax(ax,\n",
    "         'A dartboard',\n",
    "         'y',\n",
    "         'x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you look at the following graph, we represent the results of two people throwing multiple darts by their mean.\n",
    "- Which one of those two players is better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 300\n",
    "\n",
    "u_1 = [1.0, 1.]\n",
    "sigma_1 = [0.2, 0.2]\n",
    "\n",
    "X1 = np.array([np.random.normal(u_1[0], sigma_1[0], n),\n",
    "              np.random.normal(u_1[1], sigma_1[1], n)])\n",
    "\n",
    "u_2 = [-1.0, 1.]\n",
    "sigma_2 = [0.4, 0.4]\n",
    "X2 = np.array([np.random.normal(u_2[0], sigma_2[0], n),\n",
    "              np.random.normal(u_2[1], sigma_2[1], n)])\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5,5))\n",
    "ax.scatter(u_1[0], u_1[1], zorder=3, edgecolors='b')\n",
    "ax.scatter(u_2[0], u_2[1], zorder=3, edgecolors='b')\n",
    "scat1 = ax.scatter(X1[0,:], X1[1,:], zorder=2, alpha = 0.2)\n",
    "scat2 = ax.scatter(X2[0,:], X2[1,:], zorder=2, alpha = 0.2)\n",
    "\n",
    "\n",
    "draw_dartboard(ax)\n",
    "\n",
    "def update(is_display = False):\n",
    "    if(is_display):\n",
    "        scat1.set_visible(True)\n",
    "        scat2.set_visible(True)\n",
    "    else:\n",
    "        scat1.set_visible(False)\n",
    "        scat2.set_visible(False)\n",
    "    \n",
    "widgets.interact(update);\n",
    "pretty_ax(ax,\n",
    "         'A dartboard',\n",
    "         'y',\n",
    "         'x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can say that both players are equally **accurate**.\n",
    "Accurate means that they are equally close to the center of the target.\n",
    "In other words, when we talk about accuracy, you need to think about the mean.\n",
    "Now, check the box `is_display`.\n",
    "\n",
    "- Which of the two players is better?\n",
    "\n",
    "Clearly, the blue player is better.\n",
    "We can say that the blue player is more precise.\n",
    "In other words, when we talk about precision, you need to think about the variance.\n",
    "Accuracy and precision are independent of each other.\n",
    "You can be excessively accurate, but have a poor precision. \n",
    "\n",
    "For example, robotic arms typically report precision instead of accuracy.\n",
    "An arm with high precision means that it is high repeatable, but when given commands in global coordinates, might be off the expected pose.\n",
    "This is why a lot of cobots (i.e., collaborative robots) are programmed by putting the arm somewhere where and then ask to repeat the task.\n",
    "Here is the arm from [Mecademic](https://www.mecademic.com/), a company from Montreal making miniature arms, demonstrating well the concept of precision.\n",
    "We don't know if the arm of the video can be commanded to go to a global position without error, we simply know that it can always go back to a position with high precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('Aagm-z4nvRo', width=720, height=480)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robust statistic\n",
    "\n",
    "We often want to project multidimensional results, 2D in our example of a dartboard, in one dimension to simplify the interpretation.\n",
    "A typical way of doing so, it to compute an error $e$ (i.e., a scalar) based on the Euclidean distance from a result $\\vec{x}$ to a ground truth value $\\vec{g}$, such that\n",
    "\n",
    "\\begin{aligned}\n",
    "e \n",
    "&=  \\lVert \\vec{x} - \\vec{g} \\rVert_2 \\\\\n",
    "&=  \\sqrt{\\sum_j^d ( x_j - g_j} )^2\n",
    "\\textcomma\n",
    "\\end{aligned}\n",
    "\n",
    "where both vectors $\\vec{x}$ and $\\vec{g}$ are $d$-dimensional vectors $\\in \\real^n$ and where $x_j$ and $g_j$ are individual component of each vector.\n",
    "The is also known as $\\L^2$-norm, thus the notation $\\lVert \\cdot \\rVert_2 $.\n",
    "We will dig deeper in the concept of distances later when learning about point cloud registrations.\n",
    "For now, assume that $\\vec{x}$ is the 2D coordinates of one dart and that $\\vec{g}$ is at the origin of the dartboard.\n",
    "\n",
    "When we have a set with multiple trials $\\mathcal{X} = \\{ \\vec{x}_1, \\vec{x}_2, \\cdots, \\vec{x}_n \\}$, we can report the root mean squared error $\\bar{e}$ over all trials using \n",
    "\n",
    "\\begin{aligned}\n",
    "\\bar{e}\n",
    "&=  \\E \\big[ \\lVert \\mathcal{X} - \\vec{g} \\rVert_2 \\big] \\\\\n",
    "&=  \\frac{1}{n} \\sum_i^n \\lVert \\vec{x}_i - \\vec{g} \\rVert_2 \\\\\n",
    "&=  \\frac{1}{n} \\sum_i^n \\sqrt{\\sum_j^d ( x_j - g_j} )^2\n",
    "\\textdot\n",
    "\\end{aligned}\n",
    "\n",
    "Note here that $\\mathcal{X}$ is now a set composed of vectors instead of scalars used in prior sections.\n",
    "\n",
    "\n",
    "Another way to achieve the same result is to build a second set of **residual errors** $\\mathcal{E} = \\{ \\vec{e}_1, \\vec{e}_2, \\cdots, \\vec{e}_n  \\}$, where each $\\vec{e}_n = (\\vec{x}_i - \\vec{g})$. \n",
    "This representation will be useful when explaining the optimization process to register point clouds.\n",
    "Then, we can simply write\n",
    "\n",
    "\\begin{aligned}\n",
    "\\bar{e}\n",
    "&=  \\E \\big[ \\lVert \\mathcal{E} \\rVert_2 \\big] \n",
    "\\textdot\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "\n",
    "Lets reuse our dartboard example from the last section and explicit how the different throws were modeled.\n",
    "The dartboard is on the $xy$-plane, so our observation set $\\mathcal{X}$ will have 2D vectors.\n",
    "We can formalize the construction of the set by saying $\\vec{x} \\sim \\normal(\\vecsim{\\mu}, \\matsim{\\Sigma})$, where $\\normal$ is a normal distribution, $\\vecsim{\\mu} \\in \\real^2$, and $\\matsim{\\Sigma} = \\mat{I}\\vecsim{\\sigma}$, with $\\sigma \\in \\real^2$.\n",
    "Another way to explain it, it's simply to say that $\\mathcal{X}$ is build from two independant normal distribution.\n",
    "The cardinality of our set is $|\\mathcal{X}| = n$, which means that we have $n$ observations in our set.\n",
    "\n",
    "Use the following graph to get use to the set up.\n",
    "Play with the sliders and try to answer:\n",
    "- is the distribution of $\\mathcal{E}$, our error set, following a Gaussian function? \n",
    "The histogram of $\\mathcal{E}$  is at the lower-right corner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 3000\n",
    "u_x = 0.1; sigma_x = 0.4\n",
    "u_y = 1.0; sigma_y = 0.2\n",
    "\n",
    "X = np.array([np.random.normal(u_x, sigma_x, n),\n",
    "              np.random.normal(u_y, sigma_y, n)])\n",
    "E = np.linalg.norm(X, axis=0)\n",
    "\n",
    "plt.close(fig)\n",
    "fig, axs = plt.subplots(nrows=2, ncols=2, figsize=(5,5))\n",
    "scat = axs[0,0].scatter(X[0,:], X[1,:], zorder=2, alpha = 0.6,\n",
    "                        s=10,\n",
    "                        color='tab:blue')\n",
    "draw_dartboard(axs[0,0])\n",
    "pretty_ax(axs[0,0],\n",
    "         'A dartboard', 'y', 'x')\n",
    "\n",
    "\n",
    "# y\n",
    "_, _, bars_y = axs[0,1].hist(X[1,:], bins=50, \n",
    "                                    orientation=\"horizontal\")\n",
    "axs[0,1].set_ylim((-3., 3.))\n",
    "pretty_ax(axs[0,1], 'Histogram of $\\mathcal{X}$', 'y', 'count')\n",
    "# x\n",
    "_, _, bars_x = axs[1,0].hist(X[0,:], bins=50)\n",
    "axs[1,0].set_xlim((-3., 3.))\n",
    "axs[1,0].invert_yaxis()\n",
    "pretty_ax(axs[1,0], 'Histogram of $\\mathcal{X}$', 'count', 'x')\n",
    "# E \n",
    "_, _, bars_e = axs[1,1].hist(E, bins=50)\n",
    "axs[1,1].set_xlim((0., 3.))\n",
    "pretty_ax(axs[1,1], 'Histogram of $\\mathcal{E}$', 'count', 'e')\n",
    "\n",
    "def update(u_x = 0., sigma_x = 0.4, u_y = 0., sigma_y = 0.4):\n",
    "    global bars_y, bars_x, bars_e\n",
    "    X = np.array([np.random.normal(u_x, sigma_x, n),\n",
    "                  np.random.normal(u_y, sigma_y, n)])\n",
    "    E = np.linalg.norm(X, axis=0)\n",
    "    \n",
    "    scat.set_offsets(X.T)\n",
    "    scat.set_visible(True)\n",
    "    \n",
    "    # y\n",
    "    t = [b.remove() for b in bars_y]\n",
    "    count, bins, bars_y = axs[0,1].hist(X[1,:], bins=50, \n",
    "                                        orientation=\"horizontal\",\n",
    "                                       color='tab:blue')\n",
    "    # x\n",
    "    t = [b.remove() for b in bars_x]\n",
    "    count, bins, bars_x = axs[1,0].hist(X[0,:], bins=50, \n",
    "                                        color='tab:blue')\n",
    "    # E\n",
    "    t = [b.remove() for b in bars_e]\n",
    "    count, bins, bars_e = axs[1,1].hist(E, bins=50,\n",
    "                                       color='tab:blue')\n",
    "        \n",
    "    fig.canvas.draw_idle()\n",
    "\n",
    "widgets.interact(update, u_x = (-2.5, 2.5, 0.1), sigma_x = (0.1, 1.5, 0.1), u_y = (-2.5, 2.5, 0.1), sigma_y = (0.1, 1.5, 0.1));\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem here is that, eventhough we use normal distributions to construct $\\mathcal{X}$, after computing the Euclidean distance the distribution of $\\mathcal{E}$ is asymetric.\n",
    "This asymetry is particulary apparent when variances are large and the means are close to zero.\n",
    "Try it yourself with the following values for the sliders:\n",
    "- `u_x`: 0.00\n",
    "- `sigma_x`: 1.00\n",
    "- `u_y`: 0.00\n",
    "- `sigma_y`: 0.20\n",
    "\n",
    "At this point, we need to make a difference between the **mode** of a distribution and the mean:\n",
    "- mode: the most probable value of a distribution (i.e., the peak)\n",
    "- mean: the center of mass of a distribution\n",
    "\n",
    "What confuses people is that the mean and the mode are the same for symmetric distributions (e.g., uniform, normal), but can differ highly for asymmetric distributions.\n",
    "For example, a Cauchy distribution has no mean, but has a mode.\n",
    "This distribution is known has _heavy tail_ or _long tail_, so much that we need to truncate each extremity to plot it, as in the following plot.\n",
    "- comment the line where we truncate the values before plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000000\n",
    "x_lim = 2.\n",
    "\n",
    "X = np.random.standard_cauchy(n)\n",
    "\n",
    "# truncate distribution so it plots well\n",
    "X = X[(X>-x_lim) & (X<x_lim)]  # comment to see what happen!\n",
    "\n",
    "plt.close(fig)\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(5,3))\n",
    "ax.hist(X, bins=50);\n",
    "pretty_ax(ax, 'Histogram of a Cauchy distribution', 'count', 'x')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some examples of modes, means, median, standard deviation (std), and  [interquartile ranges (IQR)](https://en.wikipedia.org/wiki/Interquartile_range) for two distributions.\n",
    "We can see that mean and variance can cause problem when one assumes that it is the same as the mode for a skew distribution.\n",
    "This problem is often observed in scientific paper adding error bars to their results with the lower error bar being negative for a distance (i.e., something that can only be positive)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000000\n",
    "u_1 = 0.\n",
    "sigma_1 = 2.\n",
    "u_2 = 3.\n",
    "sigma_2 = 1.\n",
    "\n",
    "X1 = np.random.normal(u_1, sigma_1, n)\n",
    "mean1 = np.mean(X1)\n",
    "var1 = np.var(X1)\n",
    "quart1 = np.quantile(X1, [0.25, 0.5, 0.75])\n",
    "\n",
    "X2 = np.random.lognormal(u_1, sigma_2, n)\n",
    "mean2 = np.mean(X2)\n",
    "var2 = np.var(X2)\n",
    "quart2 = np.quantile(X2, [0.25, 0.5, 0.75])\n",
    "\n",
    "# truncate distribution so it plots well\n",
    "X = X[(X>-x_lim) & (X<x_lim)]  # comment to see what happen!\n",
    "\n",
    "plt.close(fig)\n",
    "fig, axs = plt.subplots(nrows=2, ncols=1, figsize=(5,5))\n",
    "\n",
    "# normal\n",
    "n, bins, _, = axs[0].hist(X1, bins=50);\n",
    "mode1 = (bins[np.argmax(n)] + bins[np.argmax(n)+1])/2.\n",
    "axs[0].axvline(mode1, color='white', linewidth=2);\n",
    "axs[0].plot(mean1, np.max(n)/5, 'o', color='tab:red');\n",
    "axs[0].plot(quart1[1], np.max(n)/3, 'o', color='tab:blue');\n",
    "axs[0].annotate('', xy=(mean1 + var1/2., np.max(n)/5),\n",
    "             xytext=(mean1 - var1/2., np.max(n)/5),\n",
    "             arrowprops=dict(arrowstyle= '|-|', color='tab:red', lw=2)\n",
    "           )\n",
    "axs[0].annotate('', xy=(quart1[2], np.max(n)/3),\n",
    "             xytext=(quart1[0], np.max(n)/3),\n",
    "             arrowprops=dict(arrowstyle= '|-|', color='tab:blue', lw=2)\n",
    "           )\n",
    "pretty_ax(axs[0], 'Histogram of a normal distribution', 'count', 'x')\n",
    "axs[0].legend(['mode', 'mean, std', 'median, IQR'])\n",
    "\n",
    "#log-normal\n",
    "X2 = X2[(X2<4.5)] # truncate distribution so it plots well\n",
    "n, bins, _, = axs[1].hist(X2, bins=50);\n",
    "mode2 = (bins[np.argmax(n)] + bins[np.argmax(n)+1])/2.\n",
    "axs[1].axvline(mode2, color='white', linewidth=2);\n",
    "axs[1].plot(mean2, np.max(n)/5, 'o', color='tab:red');\n",
    "axs[1].plot(quart2[1], np.max(n)/3, 'o', color='tab:blue');\n",
    "\n",
    "axs[1].annotate('', xy=(mean2 + var2/2., np.max(n)/5),\n",
    "             xytext=(mean2 - var2/2., np.max(n)/5),\n",
    "             arrowprops=dict(arrowstyle= '|-|', color='tab:red', lw=2)\n",
    "           )\n",
    "axs[1].annotate('', xy=(quart2[2], np.max(n)/3),\n",
    "            xytext=(quart2[0], np.max(n)/3),\n",
    "             arrowprops=dict(arrowstyle= '|-|', color='tab:blue', lw=2)\n",
    "           )\n",
    "pretty_ax(axs[1], 'Histogram of a log-normal distribution', 'count', 'x')\n",
    "axs[1].set_xlim([-1., 4.5])\n",
    "axs[1].legend(['mode', 'mean, std', 'median, IQR'])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last graph, a log-normal distribution is only defined for positive values.\n",
    "If we blindly report the mean and the std for this kind of distribution, we would give the impression that $x_i$ could be negative.\n",
    "\n",
    "\n",
    "When we want to use the mean to estimate the mode, we can use robust statistics to lower the impact of outliers.\n",
    "The arithmetic mean is not a robust estimator.\n",
    "That means that, if a single point in the set to approach infinity, the mean will also approach infinity.\n",
    "This is not the case for the median, which is a robust estimator.\n",
    "Another way to handle outliers, is to come back to our definition of the expectation.\n",
    "Recall that\n",
    "\n",
    "\\begin{equation}\n",
    "\\E[\\mathcal{X}] = \\sum_i^n p_i x_i  \n",
    "\\textdot\n",
    "\\end{equation}\n",
    "\n",
    "Instead of assuming a uniform distribution for all elements in a set, we can make a better assumption about the probability $p_i$.\n",
    "For example, with better knowledge about the problem at hand, one could define a better range for the uniform distribution by trimming a percentage of the tail (i.e., trimmed mean), or use any known probability distribution such as the normal distribution as a prior.\n",
    "The following graphs show how the original distribution get modified when using different prior for $p_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 300000\n",
    "u_x = 0.1; sigma_x = 1.4\n",
    "u_y = 2.5; sigma_y = 2.8\n",
    "\n",
    "X = np.array([np.random.normal(u_x, sigma_x, n),\n",
    "              np.random.normal(u_y, sigma_y, n)])\n",
    "\n",
    "# compute the set of error to the orgin\n",
    "E = np.linalg.norm(X, axis=0)\n",
    "\n",
    "# weights using uniform distribution\n",
    "a = 0.\n",
    "b = 4.\n",
    "p1 = ((E > a) * (E < b))/len(E)\n",
    "\n",
    "# weights using normal distribution\n",
    "mu = 0.\n",
    "sigma = 2.7\n",
    "p2 = np.exp( - (E - mu)**2 / (2 * sigma**2) )\n",
    "\n",
    "plt.close(fig)\n",
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "fig, axs = plt.subplots(nrows=3, ncols=1, figsize=(5,6))\n",
    "\n",
    "\n",
    "n, bins, _, = axs[0].hist(E, bins=50, color=colors[0], density=True, alpha=1.)\n",
    "mode = (bins[np.argmax(n)] + bins[np.argmax(n)+1])/2.\n",
    "\n",
    "ax = axs[0]\n",
    "ax.axvline(mode, color='white', linewidth=2);\n",
    "ax.plot(np.mean(E), np.max(n)/5, 'o', color='tab:red');\n",
    "pretty_ax(ax, 'Histogram of Euclidean distance', 'count', 'e')\n",
    "ax.legend(['mode', 'mean'])\n",
    "\n",
    "ax = axs[1]\n",
    "ax.hist(E, bins=50, weights=p1, color=colors[2], density=True, alpha=1.)\n",
    "ax.axvline(mode, color='white', linewidth=2);\n",
    "ax.plot(np.average(E, weights=p1), np.max(n)/5, 'o', color='tab:red');\n",
    "pretty_ax(ax, 'Robust estimation with uniform distribution', 'count', 'e')\n",
    "ax.legend(['original mode', 'weighted mean'])\n",
    "\n",
    "ax = axs[2]\n",
    "ax.hist(E, bins=50, weights=p2, color=colors[4], density=True, alpha=1.)\n",
    "ax.axvline(mode, color='white', linewidth=2);\n",
    "ax.plot(np.average(E, weights=p2), np.max(n)/5, 'o', color='tab:red');\n",
    "pretty_ax(ax, 'Robust estimation with gaussian distribution', 'count', 'e')\n",
    "ax.legend(['original mode', 'weighted mean'])\n",
    "\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Actions:\n",
    "- add notes and annotations in the markdown to help you review the material later\n",
    "- run the code in your notebook and play with the parameters to understand the behavior of the computation show as examples.\n",
    "- do the [exercises](../../exercises/uncertainty/1e-exercises_descriptive_statistics.ipynb) related to this lesson.\n",
    "- modify the markdown by adding your own notes using `> my notes` to help you review the material later; and\n",
    "- complete the tables [Symbol definitions](#Symbol-definitions) and [Glossary](#Glossary) and add your own definitions.\n",
    "\n",
    "At the end, you should be able to \n",
    "- relate: expectation, mean, raw moment, center of mass, accuracy\n",
    "- relate: expectation, variance, central moment, spread, precision\n",
    "- differ: mode, mean, median\n",
    "- understand: why the mean is not robust\n",
    "- apply: use `numpy` and `matplotlib`\n",
    "- apply: use `latex` for mathematical demonstrations\n",
    "\n",
    "Next lesson:\n",
    "- [Uncertainty in 3D](2-lesson_uncertainty.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symbol definitions\n",
    "\n",
    "| Symbol             | Definition            |\n",
    "|--------------------|-------------          |\n",
    "| $\\mathcal{X}$      | set                   |\n",
    "| $|\\mathcal{X}|$    | cardinality of a set (i.e., how many elements in a set) |\n",
    "| $x$                | scalar                |\n",
    "| $\\vec{x}$          | vector                |\n",
    "| $x_i$              | $i^{\\text{th}}$ scalar of a set                |\n",
    "| $\\real$       | real set              |\n",
    "| $\\normal$       | normal distribution              |\n",
    "| $\\E[\\mathcal{X}]$       | expectation of the set $\\mathcal{X}$             |\n",
    "| $\\in$              | ... is part of ...           |\n",
    "| ...             |            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Glossary\n",
    "\n",
    "| English             | Français             | Definition |\n",
    "|-----------          |------------          |:--------   |\n",
    "| expectation         | espérance            |            |\n",
    "| raw moment          | moment ordinaire     |            |\n",
    "| central moment      | moment centré        |            |\n",
    "| standardized moment | moment centré réduit |            |\n",
    "| precision           | précision            |            |\n",
    "| accuracy            | exactitude           |            |\n",
    "| outliers            | valeurs aberrantes   |            |\n",
    "| ...                 |                      |            |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}