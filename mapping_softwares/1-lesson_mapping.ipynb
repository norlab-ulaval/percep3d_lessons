{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;font-size: 40pt\">Mapping node</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview \n",
    "\n",
    "Objectives of this lesson:\n",
    "- explain how a 3D mapping node in ROS works\n",
    "- assist students in the construction of their first 3D map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this lesson, we are going to look at [ethzasl_icp_mapper](https://github.com/ethz-asl/ethzasl_icp_mapping/tree/reintegrate/master_into_indigo_devel/ethzasl_icp_mapper), which is a 3D mapping node in ROS. This node is written in C++ and uses the Iterative Closest Point (ICP) algorithm in the background to register point clouds. In the first part of the lesson, we will look at the way this node works. Then, we are going to build a 3D map using this mapper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic pipeline\n",
    "Although it might look complicated, in reality, it is quite easy to understand how a 3D mapper works. Here is an illustration of what happens inside:\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "<img src=\"images/simple_mapper.png\" width=\"90%\">\n",
    "</p>\n",
    "\n",
    "First, as input, the mapper takes a lidar point cloud and current [odometry](https://en.wikipedia.org/wiki/Odometry) information. \n",
    "The point cloud is used to build the map and to localize in it. \n",
    "The odometry information is used as a prior (i.e., an estimation) for localization. \n",
    "As output, the mapper produces a refined odometry as well as a map. \n",
    "The outputted map is used in the next iteration of mapping, where a new input point cloud and new odometry information are fed to the mapper.\n",
    "\n",
    "Let's look more carefully at the *Registration* box inside the mapper. \n",
    "As you've seen in the [previous module on registration](../registration/0-overview.ipynb), a registration algorithm takes two point clouds and finds the transformation that minimizes the alignment error between them. \n",
    "Here, the registration algorithm takes two point clouds and a transformation. \n",
    "This transformation is only used to roughly pre-align the point clouds, to make the registration job easier. \n",
    "Then, the registration algorithm just has to fine-tune this alignment. \n",
    "The two input point clouds are the latest point cloud produced by the lidar and the current map. \n",
    "The output of this step is a transformation representing the corrected localization of the lidar within the map.\n",
    "\n",
    "Now, let's have a look at the *Transformation* box. \n",
    "The only purpose of this step is to take the input point cloud and to align it to the map by applying the transformation computed previously. \n",
    "The output of this step is the transformed input point cloud.\n",
    "\n",
    "The last box remaining is *Merge and maintenance*. \n",
    "This step takes the current map and the transformed input point cloud and merges them. \n",
    "This step also includes maintenance jobs, such as identifying sections of the map which are not relevant (e.g., moving objects). \n",
    "The output of this step is the new current map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complete pipeline\n",
    "Now that you understand the basics, let's add two optional steps:\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "<img src=\"images/complicated_mapper.png\" width=\"90%\">\n",
    "</p>\n",
    "\n",
    "The *Input filters* box is a pre-processing step. \n",
    "It applies a series of <tt>data filters</tt>, which were introduced in the [processing lesson](../registration/2-lesson_processing.ipynb) of the previous module, to the input point cloud.\n",
    "A common use case for this step is to remove portions of the vehicle seen by the lidar from input clouds. \n",
    "If those portions are not removed, there will be a trail of points on the path of the vehicle in the map. \n",
    "The output of this step is the filtered input point cloud.\n",
    "\n",
    "The *Map post filters* box is a post-processing step. \n",
    "It applies a series of <tt>data filters</tt> to the map produced at the end of an iteration. \n",
    "A common use case for this step is to remove the sections of the map that were marked as non-relevant in the *Merge and maintenance* step. \n",
    "The output of this step is the new current map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration files\n",
    "\n",
    "The `ethzasl_icp_mapper` is a ROS node that allows users to set many options to customize the mapping.\n",
    "This node is already compiled on your virtual machine.\n",
    "We will inspect together an example of a configuration file used during the [DARPA Subterranean Challenge](https://www.subtchallenge.com/) in 2020.\n",
    "To find the appropriate configuration files, navigate to the following repository:\n",
    "\n",
    "```bash\n",
    "$ roscd ethzasl_icp_mapper/launch/husky/\n",
    "```\n",
    "\n",
    "The launch file `darpa_dynamic_mapper_ctu.launch` is the starting point for all the other files.\n",
    "With this launch file, it is possible to set the [tf](../ros/4-lesson-ros-tf.ipynb) frame names, the sensor max range, the wanted resolution for the map, the probability of points being dynamic, an so on. \n",
    "Whitin this long list of options, it is possible to provide three different configuration files. \n",
    "These configuration files are used to control what is happening in the *Input filters*, *Registration* and *Map post filters* steps. \n",
    "On the virtual machine, those three configuration files are:\n",
    "\n",
    "- `darpa_input_filters_ctu.yaml`\n",
    "- `darpa_icp_param_ctu.yaml`\n",
    "- `darpa_mapPost_filters_ctu.yaml`\n",
    "\n",
    "Here is an illustration of where those configuration files are used:\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "<img src=\"images/mapper.png\" width=\"90%\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input filters\n",
    "Here is the non-commented content of `darpa_input_filters_ctu.yaml`:\n",
    "```yaml\n",
    "- BoundingBoxDataPointsFilter:\n",
    "    xMin: -0.3\n",
    "    xMax: 10\n",
    "    yMin: -0.1\n",
    "    yMax: 0.2\n",
    "    zMin: -0.2\n",
    "    zMax: 5\n",
    "    removeInside: 1\n",
    " \n",
    "- SurfaceNormalDataPointsFilter:\n",
    "    knn: 12\n",
    "    epsilon: 1.33\n",
    "    keepNormals: 1\n",
    "    keepDensities: 1\n",
    "    \n",
    "- MaxDensityDataPointsFilter:\n",
    "    maxDensity: 1000.0\n",
    "    \n",
    "- ShadowDataPointsFilter:\n",
    "    eps: 0.1\n",
    "    \n",
    "- SimpleSensorNoiseDataPointsFilter:\n",
    "    sensorType: 0\n",
    "\n",
    "- ObservationDirectionDataPointsFilter\n",
    "\n",
    "- OrientNormalsDataPointsFilter:\n",
    "    towardCenter: 1\n",
    "```\n",
    "\n",
    "We will explain the motivation for all of those filters, with links to there specific documentation.\n",
    "The [`BoundingBoxDataPointsFilter`](https://libpointmatcher.readthedocs.io/en/latest/DataFilters/#boundingboxhead) is there to remove portions of the vehicle seen by the lidar. \n",
    "The [`SurfaceNormalDataPointsFilter`](https://libpointmatcher.readthedocs.io/en/latest/DataFilters/#surfacenormalhead) is there to compute the density and normals associated to each point. \n",
    "The density information is needed because the point cloud is then filtered based on its density by the [`MaxDensityDataPointsFilter`](https://libpointmatcher.readthedocs.io/en/latest/DataFilters/#maxdensityhead). \n",
    "As for the normals at each point, they must be computed since a Point-to-Plane error minimizer is used, as you will see in the next section.\n",
    "The [`ShadowDataPointsFilter`](https://libpointmatcher.readthedocs.io/en/latest/DataFilters/#shadowpointhead) removes points with high probability of being generated by two returns as explained in the [lesson on lidars](../autonomous_vehicles/5-lesson_lidars.ipynb#Shadow-points).\n",
    "The [`SimpleSensorNoiseDataPointsFilter`](https://libpointmatcher.readthedocs.io/en/latest/DataFilters/#sensornoisehead) adds a descriptor representing the pessimistic uncertainty between the depth uncertainty and the beam divergence.\n",
    "These values are taken from lidar specifications as also seen in the [lesson on lidars](../autonomous_vehicles/5-lesson_lidars.ipynb#Lidar-specifications).\n",
    "The field `sensorType: 0` simply means that is the noise model from the brand Sick LMS-1xx.\n",
    "The [`ObservationDirectionDataPointsFilter`](https://libpointmatcher.readthedocs.io/en/latest/DataFilters/#obsdirectionhead) adds a descriptor of where was the center of the sensor for each point.\n",
    "Finally, the [`OrientNormalsDataPointsFilter`](https://libpointmatcher.readthedocs.io/en/latest/DataFilters/#orientnormalshead) uses the information from the position of the lidar to produce consistent surface normal vector pointing inward surfaces.\n",
    "This is required as surface normal estimation using eigendecompositions, as seen in the [lesson on differencial geometry](../uncertainty/3-lesson_diff_geo.ipynb#Extract-surface-tangents), as the downside of producing the same results if the normal vector is rotated by half a turn. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ICP parameters\n",
    "\n",
    "One of the advantages of the registration library, [libpointmatcher](https://github.com/ethz-asl/libpointmatcher), used with that mapping node is that the full ICP pipeline, has explained in our [lesson on ICP overview](../registration/1-lesson_overview.ipynb#Algorithm-Overview), is configurable by a simple text file.\n",
    "Here is the non-commented content of `darpa_icp_param_ctu.yaml`:\n",
    "\n",
    "```yaml\n",
    "matcher:\n",
    "  KDTreeMatcher:\n",
    "    knn: 10\n",
    "    maxDist: 1.5\n",
    "    epsilon: 1\n",
    "\n",
    "outlierFilters:\n",
    "  - TrimmedDistOutlierFilter:\n",
    "    ratio: 0.80\n",
    "  - SurfaceNormalOutlierFilter:\n",
    "    maxAngle: 0.42\n",
    "\n",
    "errorMinimizer:\n",
    "  PointToPlaneErrorMinimizer:\n",
    "\n",
    "transformationCheckers:\n",
    "  - DifferentialTransformationChecker:\n",
    "      minDiffRotErr: 0.001\n",
    "      minDiffTransErr: 0.01\n",
    "      smoothLength: 2\n",
    "  - CounterTransformationChecker:\n",
    "      maxIterationCount: 100\n",
    "  - BoundTransformationChecker:\n",
    "      maxRotationNorm: 0.5\n",
    "      maxTranslationNorm: 2\n",
    "      \n",
    "inspector:\n",
    "  NullInspector\n",
    "\n",
    "logger:\n",
    "  FileLogger\n",
    "```\n",
    "At the top of the file, the kind of *matcher* to use is chosen. \n",
    "Matchers were introduced as association solvers in the [association lesson](../registration/2-lesson_association.ipynb) of the module on registration. \n",
    "Here, a `KDTreeMatcher` matching with the 10 closest neighbors of each point is chosen. \n",
    "Then, just below, the kind of outlier filter is chosen. \n",
    "Outlier filters were covered in the [outliers lesson](../registration/3-lesson_outliers.ipynb). \n",
    "After that, the `PointToPlaneErrorMinimizer` is set as error minimizer. \n",
    "This kind of error minimizer minimizes the distance between a point in the reading cloud and the plane which is the closest in the reference cloud. \n",
    "Error minimizers were also covered in the [error minimizer lesson](../registration/4-lesson_error_minimization.ipynb). \n",
    "Finally, convergence conditions are stated in the `transformationCheckers`.\n",
    "These conditions are used to stop the iterative process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map post filters\n",
    "\n",
    "Similary to the input filters, here is the non-commented content of `darpa_mapPost_filters_ctu.yaml`:\n",
    "\n",
    "```yaml\n",
    "- SurfaceNormalDataPointsFilter:\n",
    "    knn: 15\n",
    "\n",
    "- CutAtDescriptorThresholdDataPointsFilter:\n",
    "    descName: probabilityDynamic\n",
    "    useLargerThan: 1\n",
    "    threshold: 0.5\n",
    "```\n",
    "\n",
    "The first filter is used to re-compute the normals for each point of the map. \n",
    "Then, points which were marked as dynamic (i.e., with a probability of being dynamic higher than 50 %) are removed from the map. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demonstration\n",
    "\n",
    "Now that you understand how the mapper works, it is time to try it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data\n",
    "\n",
    "You can download a rosbag recorded during the competition in a unfinished nuclear power plant in Elma, Washington, USA.\n",
    "The file is less than 700 MB and should take less than four minutes to download.\n",
    "In the terminal of your virtual machine, run the following commands:\n",
    "\n",
    "```bash\n",
    "$ cd ~/percep3d_data/\n",
    "$ wget -O bag.zip https://ulavaldti-my.sharepoint.com/:u:/g/personal/spdes4_ulaval_ca/EQnmnAVUpvFCsfbMSdOBhM8BrI9i7ZAafGzGDEdwJyOcGQ?download=1\n",
    "$ unzip bag.zip\n",
    "$ rm bag.zip\n",
    "```\n",
    "\n",
    "If everything goes as expected, you should end up with a file named `ugv_ctu_2020-02-26-19-44-51_alpha_course_percep3d.bag` in your `~/percep3d_data/` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Launching the mapping\n",
    "In the terminal of your virtual machine, start a roscore:\n",
    "\n",
    "```bash\n",
    "$ roscore\n",
    "```\n",
    "\n",
    "Then, in another tab (press `CTRL+SHIFT+T` to open a new tab), run the following commands:\n",
    "\n",
    "```bash\n",
    "$ rosparam set use_sim_time true\n",
    "$ roslaunch ethzasl_icp_mapper darpa_dynamic_mapper_ctu.launch\n",
    "```\n",
    "\n",
    "This will start the mapping node. However, nothing will happen because no data is published yet. In another tab, open rviz:\n",
    "\n",
    "```bash\n",
    "$ rviz\n",
    "```\n",
    "\n",
    "We will configure it to allow you to see better what is going on when the mapping starts. First, remove the grid display, we won't need it. Then click on the `add` button, choose the `By topic` tab and add the `Odometry` display under `/icp_odom`. Do the same thing again for the `PointCloud2` display under `/point_map`. Then, in the options of the `PointCloud2` display you just added, change the `Style` option to `Points`, the `Size (Pixels)` option to `2` and the `Alpha` option to `0.3`. Your rviz window should look similar to this:\n",
    "\n",
    "<p style=\"text-align: center;\">\n",
    "<img src=\"images/rviz.png\" width=\"90%\">\n",
    "</p>\n",
    "\n",
    "Finally, in a new tab of the terminal, let the magic happen:\n",
    "\n",
    "```bash\n",
    "$ rosbag play ~/percep3d_data/ugv_ctu_2020-02-26-19-44-51_alpha_course_percep3d.bag --clock --keep-alive\n",
    "```\n",
    "\n",
    "- _Note_: If your computer is having a hard time, you can add the `--rate=0.5` to the command above to make the bagfile play at half of its rate.\n",
    "\n",
    "It might take a few seconds before you see anything in rviz, but then you should see a map being built. The bagfile which is playing is a snippet from a DARPA competition. It lasts 50 minutes in total and contains point clouds of multiple floors that the robot explored. It is only after approximatively 2 minutes that the robot starts going down the stairs. You don't need to play the whole bagfile, just go explore the map and have some fun."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting the map\n",
    "Once you are satisfied with the map, you can export it to a vtk file by running the following command in a new tab of the terminal:\n",
    "\n",
    "```bash\n",
    "$ rosservice call /dynamic_mapper/save_map \"filename:\n",
    "  data: '/home/student/Desktop/map.vtk'\"\n",
    "```\n",
    "\n",
    "Depending on the number of points in the map, it could take 30 s and you might have to be patient.\n",
    "Once the map is done being saved, you can open it in Paraview and play with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "You should do the following activities to enhance your understanding of the concepts viewed in this lesson:\n",
    "- modify the markdown by adding your own notes using `> my notes`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
