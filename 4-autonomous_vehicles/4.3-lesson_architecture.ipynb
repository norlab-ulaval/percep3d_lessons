{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: center;font-size: 40pt\">General architecture</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Introduction\n",
    "The goal of this lesson is to give you contextual information on where 3D registration algorithms and rigid transformations, the two main theoretical topics of this lecture, are used in a complete robotic system.\n",
    "Before explaining the architecture, let's start by defining few words.\n",
    "Following the Merriam-Webster dictionary, we have the following definition:\n",
    "\n",
    "Vehicle:\n",
    "> a means of carrying or transporting something\n",
    "\n",
    "Autonomous:\n",
    "> a) responding, reacting, or developing independently of the whole\n",
    ">\n",
    "> b) undertaken or carried on without outside control \n",
    "\n",
    "\n",
    "So, to be clear, an autonomous vehicle doesn't need to be a car.\n",
    "It can be a plane, a helicopter, a boat and so on.\n",
    "I tend to prefer those terms over _mobile robotics_ as a robotic arm is clearly mobile, but not so much a vehicle.\n",
    "Some key challenges of autonomous vehicles are the scale of the environment at which they need to operate and the uncertainty related to that environment.\n",
    "Another useful term to define is the word _intelligent agent_ or more often just _agent_.\n",
    "An agent is a generic term for someone or something else that should interact with you in an intelligent manner.\n",
    "\n",
    "There are many ways to divide the architecture of an autonomous vehicle, but they all converge these high level divisions:\n",
    "\n",
    "- **Perception**: can it understand its environment?\n",
    "- **Prediction**: can it estimates on short, medium and long terms how the environment will evolve?\n",
    "- **Decision**: can it select a set of actions fulfilling a goal? \n",
    "- **Action**: can it realize the planned actions?\n",
    "\n",
    "To connect with theories taught in this lecture, everything related to the Euclidean space is heavily used for all of those steps since a robot moves in 3D.\n",
    "As for the content around lidars and 3D registration, it is mostly related to the perception level.\n",
    "\n",
    "In this lesson, I curated a list of examples and added few precisions on these divisions using research videos with interesting results.\n",
    "The goal is not for you to understand the specific methods put forward, but simply to illustrate what these levels imply.\n",
    "It is important that you focus on high-level concepts instead of the technical details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perception\n",
    "\n",
    "This level can be further divided in self-localization and scene understanding.\n",
    "Scene understanding is where machine learning algorithms shined in the recent years.\n",
    "When it comes to self-localization, which will be better explained in the [next lesson](4-lesson_localization_mapping.ipynb), it is still mostly model based algorithms that are used.\n",
    "\n",
    "Some example related to new development of perception solutions are:\n",
    "- new sensor modalities, such as [event cameras (4 min)](https://www.youtube.com/watch?v=0q6ap_OSBAk).\n",
    "- [tracking objects and humans using a camera (5 min)](https://youtu.be/RByCiOLlxug).\n",
    "This video is interesting as it also highlights well general challenges of tracking.\n",
    "- [lidar segmentation of urban environments (5 min)](https://youtu.be/wuokg7MFZyU?t=98).\n",
    "- [real-time 3D reconstruction from lidar (4 min)](https://youtu.be/jlAArM_6N-I), a video of some 3D maps I did when I was at the University of Toronto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction\n",
    "\n",
    "Let's start with some examples from research videos:\n",
    "- short version (1 min): [Pedestrian prediction using personality traits](https://youtu.be/37StrvOhDbs)\n",
    "\n",
    "Currently the hardest part for autonomous cars according to Gill Pratt, the chief executive officer of Toyota Research Institute (TRI) , formerly a program manager at the Defense Advanced Research Projects Agency (DARPA); and Wolfram Burgard, vice president of automated driving technology for TRI and president of the IEEE Robotics and Automation Society, as stated by an article in [IEEE Spectrum 2020](https://spectrum.ieee.org/transportation/self-driving/qa-the-masterminds-behind-toyotas-selfdriving-cars-say-ai-still-has-a-way-to-go).\n",
    "Similar to weather forecasts, the more you look in the future, the more uncertain are your predictions.\n",
    "The problem with autonomous vehicles is that you have very little time to compute these predictions and only partial information.\n",
    "Prediction implies a very good understanding of the environment and how it will interact with other agents.\n",
    "On the one hand, this is at reach when it comes to predicting the action of your own autonomous vehicle, but still under research when it comes to harsh environment like snow or ice on the ground.\n",
    "On the other hand, predicting what a human can do is very complex.\n",
    "Currently, we can predict with good accuracy the motion of a pedestrian within less than two seconds or so.\n",
    "But, let's face it, even some humans don't know themselves what they will be doing in two seconds.\n",
    "We have flashing turn signal lights since 1938 on cars and that doesn't stop someone to cut you without warning.\n",
    "Moreover, this prediction depends of cultural bias.\n",
    "For example, jaywalking is more probable in some countries than in others.\n",
    "Finally, motion prediction of human behavior has a [Minority Report](https://youtu.be/2bvFr2ANNkM) feelings and tends not to be well accepted in the society in general."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision\n",
    "\n",
    "Decision is related to the research topic of planning and coordination with other agents.\n",
    "Planning can be divided in path planning and task planning.\n",
    "Path planning is a more active research topic in robotics as task planning is deeply covered in other fields.\n",
    "For example, task planning is used to optimize production line and or room reservations.\n",
    "It can be seen as a general constrained optimization problem.\n",
    "On the other side, path planning is more specialized as you want to find a trajectory in SE(3) that can be followed by a group of actuators.\n",
    "The more actuators you have, the larger is your search space and the longer it takes to compute.\n",
    "When you can plan your own path, you can start to coordinates with other agents to accomplish more complicated tasks.\n",
    "\n",
    "Here are some examples:\n",
    "\n",
    "- A video from Sebastian Thrun who founded [Udacity](https://en.wikipedia.org/wiki/Udacity), and explaining the path planning solution used in the third DARPA Grand Challenge [A* in Action (3.5 min)](https://www.youtube.com/watch?v=qXZt-B7iUyw)\n",
    "- Interesting way to show path planning algorithms [using a projector from MIT (9 min)](https://www.youtube.com/watch?v=14jfid-CkQo)\n",
    "[Path planning in high dimensions for robotic arms (2 min)](https://youtu.be/TQIoCC48gp4?t=57) from colleagues at the University of Toronto.\n",
    "- Robot Atlas, from Boston Dynamic, [installing dry walls (1 min)](https://youtu.be/JMLPhk6b0gA)\n",
    "On a side note, the construction of Atlas was commissioned by DARPA for their fourth challenge related to humanoids from 2013 to 2015.\n",
    "- In 2014, a laboratory from Harvard showed the [coordination of 1000 robots (2 min)](https://www.youtube.com/watch?v=2IAluwgAFD0).\n",
    "- More recently (2018), Intel pays for more engineers and technicians than a single lab can afford and could bring the count to [2000 drones in the sky (3 min)](https://www.youtube.com/watch?v=zdLjoqa_oUs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action\n",
    "\n",
    "It's not all to plan a sequence of actions, the robot also needs to be able to follow them.\n",
    "It other words, most path planning algorithms work on simulations to find the optimal solution, but there is a gap between the simulated environment and the real one that needs to be filled.\n",
    "This is where new actuator and better control algorithms intervene.\n",
    "For example, falling in that level there is:\n",
    "- [predictive control for aggressive driving (6 min)](https://youtu.be/1AR2-OHCxsQ).\n",
    "- if you have budget for advertising on top of research (I'm looking at you Stanford), why not doing the same thing with a [retrofitted DeLorean (3 min)](https://youtu.be/3x3SqeSdrAE).\n",
    "- Cheetah from Boston Dynamics [running at 45.5 km/h (1 min)](https://youtu.be/chPanW0QWhA)\n",
    "- new actuator configuration to extend workspace [from Clement Gosselin's lab (1 min)](https://youtu.be/Nv2LqH3R1uw).\n",
    "- [origami robots for very small robots (3 min)](https://youtu.be/h9ray9dJd5k)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The big picture\n",
    "\n",
    "Those levels from a general autonomous vehicle are in sequence only for very simple systems.\n",
    "Most of the time, they are all interconnected and require a very high level of integration.\n",
    "For example, if you want to put in place a type of controller named [Model Predictive Control](https://youtu.be/8U0xiOkDcmw), you will need all of those levels at high speed.\n",
    "In the context of our lecture, this division was used to give you an overview of robotic systems for your general knowledge, but also to understand that perception is a small part the overall architecture.\n",
    "\n",
    "Next lesson:\n",
    "- [Localization and mapping](4-lesson_localization_mapping.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#8635; [Go back to the list of lessons](0-overview.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
